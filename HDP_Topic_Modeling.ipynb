{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOtaXUJxZTnflrXUEYH6PD/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dibyanshu12/Topic-Modeling/blob/main/HDP_Topic_Modeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IXa1Mg4AISGT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pandas import option_context\n",
        "import numpy as np\n",
        "import string\n",
        "import re\n",
        "import sys\n",
        "\n",
        "# Sklearn\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "st_words = stopwords.words('english')\n",
        "st_words.extend(['from', 'subject', 're', 'edu', 'use'])"
      ],
      "metadata": {
        "id": "qoT3ACYeIYRr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news = fetch_20newsgroups(subset='train') # import fron skleanr"
      ],
      "metadata": {
        "id": "lzO0jhDMIeho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build master df\n",
        "\n",
        "news_df = pd.DataFrame([news.target, news.data]).T\n",
        "news_df = news_df.set_index(0)\n",
        "#news_df = pd.concat([news_df, pd.Series(news.target_names)],axis=1, join=\"inner\")\n",
        "news_df.reset_index(inplace=True)\n",
        "news_df.columns = [\"topic_id\", \"content\"]\n",
        "\n",
        "news_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mb-oxgXWIk8K",
        "outputId": "b57de5ee-b13d-4ded-87d1-12e6f56fd590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic_id</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   topic_id                                            content\n",
              "0         7  From: lerxst@wam.umd.edu (where's my thing)\\nS...\n",
              "1         4  From: guykuo@carson.u.washington.edu (Guy Kuo)...\n",
              "2         4  From: twillis@ec.ecn.purdue.edu (Thomas E Will...\n",
              "3         1  From: jgreen@amber (Joe Green)\\nSubject: Re: W...\n",
              "4        14  From: jcm@head-cfa.harvard.edu (Jonathan McDow..."
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc_list = news_df.content.values.tolist()"
      ],
      "metadata": {
        "id": "5gN1hV6kIzwq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_list[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "rl6LKt03Jx-F",
        "outputId": "3cd1ed3b-a36b-41f4-e025-9090853eda25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'From: twillis@ec.ecn.purdue.edu (Thomas E Willis)\\nSubject: PB questions...\\nOrganization: Purdue University Engineering Computer Network\\nDistribution: usa\\nLines: 36\\n\\nwell folks, my mac plus finally gave up the ghost this weekend after\\nstarting life as a 512k way back in 1985.  sooo, i\\'m in the market for a\\nnew machine a bit sooner than i intended to be...\\n\\ni\\'m looking into picking up a powerbook 160 or maybe 180 and have a bunch\\nof questions that (hopefully) somebody can answer:\\n\\n* does anybody know any dirt on when the next round of powerbook\\nintroductions are expected?  i\\'d heard the 185c was supposed to make an\\nappearence \"this summer\" but haven\\'t heard anymore on it - and since i\\ndon\\'t have access to macleak, i was wondering if anybody out there had\\nmore info...\\n\\n* has anybody heard rumors about price drops to the powerbook line like the\\nones the duo\\'s just went through recently?\\n\\n* what\\'s the impression of the display on the 180?  i could probably swing\\na 180 if i got the 80Mb disk rather than the 120, but i don\\'t really have\\na feel for how much \"better\" the display is (yea, it looks great in the\\nstore, but is that all \"wow\" or is it really that good?).  could i solicit\\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\\ntaking the disk size and money hit to get the active display?  (i realize\\nthis is a real subjective question, but i\\'ve only played around with the\\nmachines in a computer store breifly and figured the opinions of somebody\\nwho actually uses the machine daily might prove helpful).\\n\\n* how well does hellcats perform?  ;)\\n\\nthanks a bunch in advance for any info - if you could email, i\\'ll post a\\nsummary (news reading time is at a premium with finals just around the\\ncorner... :( )\\n--\\nTom Willis  \\\\  twillis@ecn.purdue.edu    \\\\    Purdue Electrical Engineering\\n---------------------------------------------------------------------------\\n\"Convictions are more dangerous enemies of truth than lies.\"  - F. W.\\nNietzsche\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Emails\n",
        "doc_list = [re.sub(r'\\S*@\\S*\\s?', '', sent) for sent in doc_list]\n",
        "\n",
        "# Remove new line characters\n",
        "doc_list = [re.sub(r'\\s+', ' ', sent) for sent in doc_list]\n",
        "\n",
        "# Remove distracting single quotes\n",
        "doc_list = [re.sub(r\"\\'\", \"\", sent) for sent in doc_list]"
      ],
      "metadata": {
        "id": "W1mJsipvJ2iB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_list[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "d1zZCbjhJ9VE",
        "outputId": "6a73cfa3-f843-44a3-bbe5-bbc52e35fb96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'From: (Thomas E Willis) Subject: PB questions... Organization: Purdue University Engineering Computer Network Distribution: usa Lines: 36 well folks, my mac plus finally gave up the ghost this weekend after starting life as a 512k way back in 1985. sooo, im in the market for a new machine a bit sooner than i intended to be... im looking into picking up a powerbook 160 or maybe 180 and have a bunch of questions that (hopefully) somebody can answer: * does anybody know any dirt on when the next round of powerbook introductions are expected? id heard the 185c was supposed to make an appearence \"this summer\" but havent heard anymore on it - and since i dont have access to macleak, i was wondering if anybody out there had more info... * has anybody heard rumors about price drops to the powerbook line like the ones the duos just went through recently? * whats the impression of the display on the 180? i could probably swing a 180 if i got the 80Mb disk rather than the 120, but i dont really have a feel for how much \"better\" the display is (yea, it looks great in the store, but is that all \"wow\" or is it really that good?). could i solicit some opinions of people who use the 160 and 180 day-to-day on if its worth taking the disk size and money hit to get the active display? (i realize this is a real subjective question, but ive only played around with the machines in a computer store breifly and figured the opinions of somebody who actually uses the machine daily might prove helpful). * how well does hellcats perform? ;) thanks a bunch in advance for any info - if you could email, ill post a summary (news reading time is at a premium with finals just around the corner... :( ) -- Tom Willis \\\\ \\\\ Purdue Electrical Engineering --------------------------------------------------------------------------- \"Convictions are more dangerous enemies of truth than lies.\" - F. W. Nietzsche '"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Gensim\n",
        "import gensim\n",
        "\n",
        "word_list = [gensim.utils.simple_preprocess(txt, deacc=True, min_len=3) for txt in doc_list]"
      ],
      "metadata": {
        "id": "gSo4AEGLKCUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_list[2][:6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6PDYxivpKHcb",
        "outputId": "ccb9be40-1011-473d-9ca7-9dc852e20fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['from', 'thomas', 'willis', 'subject', 'questions', 'organization']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Phrases\n",
        "from gensim.models.phrases import Phraser\n",
        "\n",
        "bigram = Phrases(word_list, min_count=5, threshold=100)\n",
        "\n",
        "bigram_model = Phraser(bigram)"
      ],
      "metadata": {
        "id": "h--I3T0iKXpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of stop word removal process\n",
        "sentence = \"A letter has been written asking him to be released\"\n",
        "for word in sentence.split():\n",
        "    if word not in st_words:\n",
        "        print(\"Kept ==> \" +word)\n",
        "    else:\n",
        "        print(\"Removed: \" +word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc3EFIl0KhWf",
        "outputId": "2fdc55b2-78c2-4e59-d926-cf22e82ff7a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kept ==> A\n",
            "Kept ==> letter\n",
            "Removed: has\n",
            "Removed: been\n",
            "Kept ==> written\n",
            "Kept ==> asking\n",
            "Removed: him\n",
            "Removed: to\n",
            "Removed: be\n",
            "Kept ==> released\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_list_nostops = [[word for word in txt if word not in st_words] for txt in word_list]\n",
        "word_list_nostops[2][:6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BU2RT8K5KoU9",
        "outputId": "f1e42766-4e28-4987-cb93-ff1352b0bb8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['thomas', 'willis', 'questions', 'organization', 'purdue', 'university']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_bigrams = [bigram_model[w_vec] for w_vec in word_list_nostops]"
      ],
      "metadata": {
        "id": "QoTYobAnKyQz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_bigrams[2][:7]"
      ],
      "metadata": {
        "id": "PwOCV5KxK2Yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "## Example showing lemmatization\n",
        "sentence = \"A letter has been written asking him to be released\"\n",
        "\n",
        "spC = spacy.load('en_core_web_sm')\n",
        "\n",
        "lemma_sentence = spC(sentence)\n",
        "for token in lemma_sentence:\n",
        "    print(token.text + \" ==> \" +token.lemma_ + \", \" + token.pos_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F1TJHNBTLLrz",
        "outputId": "af72a039-8883-40cb-bcab-0de705a0ede7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A ==> a, DET\n",
            "letter ==> letter, NOUN\n",
            "has ==> have, AUX\n",
            "been ==> be, AUX\n",
            "written ==> write, VERB\n",
            "asking ==> ask, VERB\n",
            "him ==> -PRON-, PRON\n",
            "to ==> to, PART\n",
            "be ==> be, AUX\n",
            "released ==> release, VERB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spC = spacy.load('en_core_web_sm')\n",
        "postags = ['NOUN','VERB', 'ADV', 'ADJ'] # Keep nouns, adj, verbs, and adverbs\n",
        "\n",
        "\n",
        "def lemmatize(word_list, ptags = postags ):\n",
        "    '''Lemmatizes words based on allowed postags, input format is list of sublists\n",
        "       with strings'''\n",
        "\n",
        "    lem_lists =[]\n",
        "    for vec in word_list:\n",
        "        sentence = spC(\" \".join(vec))\n",
        "        lem_lists.append([token.lemma_ for token in sentence if token.pos_ in ptags])\n",
        "\n",
        "    return lem_lists"
      ],
      "metadata": {
        "id": "lp-QvDCGLNEv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize bigrams\n",
        "word_list_lemmatized = lemmatize(word_bigrams)"
      ],
      "metadata": {
        "id": "IbSazNVYLRlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_list_lemmatized[2][:7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EEuEQIZhLWWT",
        "outputId": "c7bbee38-5bcb-46da-a19e-e8ade28f118a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['engineering',\n",
              " 'computer',\n",
              " 'network',\n",
              " 'distribution_usa',\n",
              " 'line',\n",
              " 'well',\n",
              " 'folk']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tomotopy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJneys4qNCiq",
        "outputId": "05768451-cd20-40d8-96fa-9e6fc940ab79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tomotopy\n",
            "  Downloading tomotopy-0.12.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (16.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 16.3 MB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tomotopy) (1.19.5)\n",
            "Installing collected packages: tomotopy\n",
            "Successfully installed tomotopy-0.12.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tomotopy as tp\n",
        "\n",
        "hdp = tp.HDPModel(tw=tp.TermWeight.IDF, min_cf=5, rm_top=7,\n",
        "                 gamma=1, alpha=0.1, initial_k=10, seed=99999)\n",
        "\n",
        "# Add docs to train\n",
        "for vec in word_list_lemmatized:\n",
        "    hdp.add_doc(vec)\n",
        "\n",
        "# Initiate MCMC burn-in\n",
        "hdp.burn_in = 100\n",
        "hdp.train(0)\n",
        "print('Num docs:', len(hdp.docs), ', Vocab size:', hdp.num_vocabs, ', Num words:', hdp.num_words)\n",
        "print('Removed top words:', hdp.removed_top_words)\n",
        "print('Training...', file=sys.stderr, flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nN-_krCkNMtu",
        "outputId": "af5a664b-cfbb-42ff-af3b-72a7175d76e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num docs: 11314 , Vocab size: 12716 , Num words: 1004667\n",
            "Removed top words: ['line', 'would', 'write', 'say', 'know', 'article', 'people']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "mcmc_iter=1000\n",
        "for i in range(0, mcmc_iter, 100):\n",
        "    hdp.train(100, workers=3)\n",
        "    print('Iteration: {}\\tLog-likelihood: {}\\tNum. of topics: {}'.format(i, hdp.ll_per_word, hdp.live_k))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "59qAewRlNS1W",
        "outputId": "da89ccd5-e019-4440-ca49-bfd1ba81d72c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0\tLog-likelihood: -8.040162289508805\tNum. of topics: 18\n",
            "Iteration: 100\tLog-likelihood: -8.036344954011918\tNum. of topics: 19\n",
            "Iteration: 200\tLog-likelihood: -8.036610465558239\tNum. of topics: 21\n",
            "Iteration: 300\tLog-likelihood: -8.03494260430024\tNum. of topics: 21\n",
            "Iteration: 400\tLog-likelihood: -8.035102852242396\tNum. of topics: 19\n",
            "Iteration: 500\tLog-likelihood: -8.034813546651522\tNum. of topics: 18\n",
            "Iteration: 600\tLog-likelihood: -8.035080041624965\tNum. of topics: 19\n",
            "Iteration: 700\tLog-likelihood: -8.03520256431882\tNum. of topics: 18\n",
            "Iteration: 800\tLog-likelihood: -8.03547803833439\tNum. of topics: 19\n",
            "Iteration: 900\tLog-likelihood: -8.035228853412798\tNum. of topics: 18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hdp_topics(hdp, top_n=10):\n",
        "    '''Wrapper function to extract topics from trained tomotopy HDP model\n",
        "\n",
        "    ** Inputs **\n",
        "    hdp:obj -> HDPModel trained model\n",
        "    top_n: int -> top n words in topic based on frequencies\n",
        "\n",
        "    ** Returns **\n",
        "    topics: dict -> per topic, an arrays with top words and associated frequencies\n",
        "    '''\n",
        "\n",
        "    # Get most important topics by # of times they were assigned (i.e. counts)\n",
        "    sorted_topics = [k for k, v in sorted(enumerate(hdp.get_count_by_topics()), key=lambda x:x[1], reverse=True)]\n",
        "\n",
        "    topics=dict()\n",
        "\n",
        "    # For topics found, extract only those that are still assigned\n",
        "    for k in sorted_topics:\n",
        "        if not hdp.is_live_topic(k): continue # remove un-assigned topics at the end (i.e. not alive)\n",
        "        topic_wp =[]\n",
        "        for word, prob in hdp.get_topic_words(k, top_n=top_n):\n",
        "            topic_wp.append((word, prob))\n",
        "\n",
        "        topics[k] = topic_wp # store topic word/frequency array\n",
        "\n",
        "    return topics"
      ],
      "metadata": {
        "id": "IkVo5qXINckE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hdp_topics = get_hdp_topics(hdp)"
      ],
      "metadata": {
        "id": "qrkwULY5Oe-M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hdp_topics[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TXSvYL8IOftQ",
        "outputId": "6943e68f-fc04-4721-e44e-746da13cf18d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('believe', 0.002693449379876256),\n",
              " ('think', 0.00257707666605711),\n",
              " ('law', 0.002481670817360282),\n",
              " ('make', 0.0024207490496337414),\n",
              " ('religion', 0.0022249806206673384),\n",
              " ('may', 0.002082108287140727),\n",
              " ('see', 0.0020164246670901775),\n",
              " ('right', 0.0019723433069884777),\n",
              " ('many', 0.00193266780115664),\n",
              " ('state', 0.0019238201202824712)]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.corpora as corpora\n",
        "from gensim.models import CoherenceModel\n",
        "\n",
        "# Build gensim objects\n",
        "vocab = corpora.Dictionary(word_list_lemmatized)\n",
        "corpus = [vocab.doc2bow(words) for words in word_list_lemmatized]\n",
        "\n",
        "# Build topic list from dictionary\n",
        "topic_list=[]\n",
        "for k, tups in hdp_topics.items():\n",
        "    topic_tokens=[]\n",
        "    for w, p in tups:\n",
        "        topic_tokens.append(w)\n",
        "\n",
        "    topic_list.append(topic_tokens)"
      ],
      "metadata": {
        "id": "8nEwoiSZOtiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic_list[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OO2pK94MOywo",
        "outputId": "5b6c55ff-6aa2-4317-c71b-31486ca8ab51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['believe',\n",
              " 'think',\n",
              " 'law',\n",
              " 'make',\n",
              " 'religion',\n",
              " 'may',\n",
              " 'see',\n",
              " 'right',\n",
              " 'many',\n",
              " 'state']"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cm = CoherenceModel(topics=topic_list, corpus=corpus, dictionary=vocab, texts=word_list_lemmatized,\n",
        "                    coherence='c_v')\n",
        "\n",
        "cm.get_coherence()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KiEsjlgMPV9G",
        "outputId": "cf8ad229-3a6b-413a-987f-ca7ae541e5b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5963344200118257"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_doc = word_list_lemmatized[0]\n",
        "\n",
        "doc_inst = hdp.make_doc(test_doc)\n",
        "\n",
        "topic_dist, ll = hdp.infer(doc_inst)\n",
        "\n",
        "topic_idx = np.array(topic_dist).argmax()\n",
        "topic_idx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PylESbgbPZ7u",
        "outputId": "4dad6540-8814-4ddc-f761-e9d4aa958863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "hdp.get_topic_words(topic_idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISCdJT3jPkQd",
        "outputId": "89e52125-91f2-4b35-dab4-c9b8100cd796"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('car', 0.006628578063100576),\n",
              " ('bike', 0.0040488168597221375),\n",
              " ('ride', 0.0026391283608973026),\n",
              " ('get', 0.0024984274059534073),\n",
              " ('engine', 0.0023395137395709753),\n",
              " ('drive', 0.002275686478242278),\n",
              " ('buy', 0.002237173495814204),\n",
              " ('good', 0.0021873542573302984),\n",
              " ('wire', 0.0021769863087683916),\n",
              " ('look', 0.0020851497538387775)]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "news_df.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eJDixTtVPorj",
        "outputId": "fd8bcbd7-881d-4932-ace1-7d1988ef1b4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>topic_id</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7</td>\n",
              "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14</td>\n",
              "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   topic_id                                            content\n",
              "0         7  From: lerxst@wam.umd.edu (where's my thing)\\nS...\n",
              "1         4  From: guykuo@carson.u.washington.edu (Guy Kuo)...\n",
              "2         4  From: twillis@ec.ecn.purdue.edu (Thomas E Will...\n",
              "3         1  From: jgreen@amber (Joe Green)\\nSubject: Re: W...\n",
              "4        14  From: jcm@head-cfa.harvard.edu (Jonathan McDow..."
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scripts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KMMvmDhgQNe6",
        "outputId": "7c51403f-6dc7-4810-f59b-7aa137cd7d93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scripts\n",
            "  Downloading scripts-2.0-py2.py3-none-any.whl (20 kB)\n",
            "Collecting stua\n",
            "  Downloading stua-0.2-py2.py3-none-any.whl (8.2 kB)\n",
            "Installing collected packages: stua, scripts\n",
            "Successfully installed scripts-2.0 stua-0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import HdpModel\n",
        "\n",
        "# Filter outlier words (e.g. appear <10 docs or in over 50% of docs)\n",
        "vocab = corpora.Dictionary(word_list_lemmatized)\n",
        "vocab.filter_extremes(no_below=20, no_above=0.5)\n",
        "vocab.compactify()\n",
        "\n",
        "# Build corpus\n",
        "corpus = [vocab.doc2bow(words) for words in word_list_lemmatized]\n",
        "\n",
        "# Just changing the learning rate and truncation level\n",
        "hdp = HdpModel(corpus, vocab, gamma=1, T=30, alpha=0.1, K=8, kappa=1, random_state=20) # default kappa\n",
        "hdp08 = HdpModel(corpus, vocab, gamma=1, T=30, alpha=0.1, K=8, kappa=0.8, random_state=20)\n",
        "hdp06 = HdpModel(corpus, vocab, gamma=1, T=30, alpha=0.1, K=8, kappa=0.6, random_state=20)"
      ],
      "metadata": {
        "id": "543sgPuOPzr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vb_model = [hdp06, hdp08, hdp]\n",
        "vb_coherence =[]\n",
        "\n",
        "for m in vb_model:\n",
        "    cm = CoherenceModel(m, texts=word_list_lemmatized, corpus=corpus, dictionary=vocab, coherence='c_v')\n",
        "    vb_coherence.append(cm.get_coherence())"
      ],
      "metadata": {
        "id": "j4JfiGFEQL9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame(vb_coherence, index=['hdp_kap06', 'hdp_kap08', 'hdp_kap1']).T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "VQLGPkqsQ4CQ",
        "outputId": "c072f8fc-8d5e-42b7-d01f-06b6d8d0f531"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>hdp_kap06</th>\n",
              "      <th>hdp_kap08</th>\n",
              "      <th>hdp_kap1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.328488</td>\n",
              "      <td>0.422874</td>\n",
              "      <td>0.499369</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   hdp_kap06  hdp_kap08  hdp_kap1\n",
              "0   0.328488   0.422874  0.499369"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GvG1uQTVRVDw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}